{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Go through the df_iemocap.csv file, for every row, use roberta ,hubert and librosa to extract features, store in .pkl file.:\n",
    "Use sliding windows, create sequence data, contain current sentence and history (past two sentence)\n",
    "in order to deal with history, use the index column.\n",
    "    if current index >=3, then HISTORY_LENGTH = 2, use past two sentences .\n",
    "    if current index <3, then history num = (index-1), \n",
    "    index=1, means current sentence is the first sentence, HISTORY_LENGTH=0, \n",
    "    index =2, means current sentence is the second sentence, HISTORY_LENGTH=1\n",
    "\n",
    "'''\n",
    "# --- Imoort libraries ---\n",
    "import os\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, HubertModel, HubertConfig\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "IEMOCAP_DIR = \"../../dataset/IEMOCAP_full_release\"  \n",
    "SESSIONS = [\"Session1\", \"Session2\", \"Session3\", \"Session4\", \"Session5\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#OUTPUT_CSV_DIR = \"data/preprocess/\"\n",
    "OUTPUT_PKL_DIR = \"../../data\"\n",
    "OUTPUT_PKL_FILENAME = \"iemocap_preprocessed_data.pkl\"\n",
    "TRANSCRIPT_CSV = \"../../data/df_iemocap.csv\"  \n",
    "HISTORY_LENGTH = 3  # Number of previous utterances to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HubertModel(\n",
       "  (feature_extractor): HubertFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): HubertGroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x HubertNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x HubertNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): HubertFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): HubertEncoder(\n",
       "    (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): HubertSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x HubertEncoderLayer(\n",
       "        (attention): HubertSdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): HubertFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Models and Tokenizers ---\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "hubert_config = HubertConfig.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\").to(DEVICE)\n",
    "hubert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Hubert to extract audio feature\n",
    "def extract_audio_features(audio_path):\n",
    "    \"\"\"Extract HuBERT features from audio.\"\"\"\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    audio_tensor = torch.tensor(audio).unsqueeze(0).to(DEVICE).float()\n",
    "    with torch.no_grad():\n",
    "        outputs = hubert_model(audio_tensor)\n",
    "    return outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "\n",
    "#  get transcript for audio file\n",
    "def get_transcription(audio_file_path, df_iemocap):\n",
    "    \"\"\"Get transcription from the provided CSV DataFrame.\"\"\"\n",
    "    audio_filename = os.path.basename(audio_file_path)\n",
    "    print (audio_filename)\n",
    "    if audio_filename in df_iemocap['wav_file'].values:\n",
    "        return df_iemocap.loc[df_iemocap['wav_file'] == audio_filename, 'script'].iloc[0]\n",
    "    else:\n",
    "        print(f\"Warning: Transcription not found for {audio_filename}\")\n",
    "        return \"Transcription not found.\"  # Or handle missing transcript as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(pkl_dir):\n",
    "    \"\"\"Load preprocessed data from .pkl file.\"\"\"\n",
    "    pkl_path = os.path.join(pkl_dir, OUTPUT_PKL_FILENAME )\n",
    "    if os.path.exists(pkl_path):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_pairs(data):\n",
    "    sequence_data = []\n",
    "    print(\"Create sequence data...\")   \n",
    "    for i in range(len(data) ):\n",
    "        current_data = data[i]   \n",
    "        print(current_data[\"index\"])\n",
    "        history = []\n",
    "\n",
    "        # first sentence no history, second sentence only has one history, third or later sentence has 2 history\n",
    "        if current_data[\"index\"]<3:\n",
    "            history_length = current_data[\"index\"]-1\n",
    "        else:\n",
    "            history_length = HISTORY_LENGTH      \n",
    "\n",
    "        for j in range(max(0, i - history_length), i):\n",
    "            history.append({\n",
    "                \"input_ids\": data[j][\"input_ids\"],\n",
    "                \"attention_mask\": data[j][\"attention_mask\"],\n",
    "                \"hubert_features\": data[j][\"hubert_features\"],               \n",
    "                \"label\": data[j][\"label\"],              \n",
    "                \"next_label\": data[j][\"next_label\"]\n",
    "            })\n",
    "        \n",
    "        print(\"hubert_features in sequence data\",current_data[\"hubert_features\"])\n",
    "        sequence_data.append({\n",
    "            \"history\": history,\n",
    "            \"current_hubert\": current_data[\"hubert_features\"],         \n",
    "            \"current_text\": current_data[\"transcription\"],\n",
    "            \"current_label\": current_data[\"label\"],\n",
    "            \"current_next_label\": current_data[\"next_label\"],\n",
    "        })\n",
    "\n",
    "    return sequence_data\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ses01F_impro01_F000\n",
      "label neu\n",
      "Create sequence data...\n",
      "1\n",
      "hubert_features in sequence data [[ 0.11579464  0.11709942  0.28083935 ... -0.2782844  -0.06476727\n",
      "   0.21543494]\n",
      " [ 0.16728824  0.15846454  0.28208348 ... -0.3612778  -0.09611124\n",
      "   0.04330719]\n",
      " [ 0.18510109  0.12920734  0.27065846 ... -0.31461972 -0.0056014\n",
      "   0.06006904]\n",
      " ...\n",
      " [-0.00815474  0.4321724   0.2144017  ... -0.04938321  0.08605168\n",
      "   0.13589515]\n",
      " [-0.02400946  0.4112573   0.21331745 ... -0.10015789  0.09740229\n",
      "   0.09691934]\n",
      " [ 0.0906674   0.31126025  0.20738426 ... -0.15112437  0.04656437\n",
      "   0.08059184]]\n",
      "Data preprocessed and saved to ../../data\\iemocap_preprocessed_data.pkl\n",
      "Example Loaded Data:\n",
      "History Length: 0\n",
      "Current Hubert Shape: (97, 768)\n",
      "Current Text Shape: Excuse me.\n",
      "Current Label: neu\n",
      "Current Next Label: fru\n"
     ]
    }
   ],
   "source": [
    "# test on 1 rows\n",
    "def test_preprocess_iemocap(df_iemocap, output_dir):\n",
    "    \"\"\"Preprocess data using audio filenames from the transcript CSV.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = []\n",
    "    \n",
    "    row = df_iemocap.iloc[0]\n",
    "    audio_filename = row['wav_file']\n",
    "    audio_subpath = audio_filename[:audio_filename.rfind(\"_\")]\n",
    "    audio_path = IEMOCAP_DIR +\"/Session\" + audio_filename[4]+ \"/sentences/wav/\"+ audio_subpath + \"/\"+ audio_filename+\".wav\"\n",
    "    #audio_path = os.path.join(IEMOCAP_DIR, \"Session\" + audio_filename[3], \"sentences\", \"wav\", audio_filename)  # Reconstruct audio path.\n",
    "        \n",
    "    if os.path.exists(audio_path):       \n",
    "       \n",
    "        transcription = get_transcription(audio_filename, df_iemocap)\n",
    "        hubert_features  = extract_audio_features(audio_path)     \n",
    "\n",
    "        text_tokens = roberta_tokenizer(transcription, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        text_tokens = {key: value.squeeze(0).cpu().numpy() for key, value in text_tokens.items()}       \n",
    "        label = row['label'] \n",
    "        index = row['index']\n",
    "        next_label = row['next_label'] \n",
    "        print(\"label\",label)\n",
    "\n",
    "        data.append({\n",
    "            \"audio_file\": audio_filename,\n",
    "            \"hubert_features\": hubert_features,           \n",
    "            \"input_ids\": text_tokens[\"input_ids\"],\n",
    "            \"attention_mask\": text_tokens[\"attention_mask\"],\n",
    "            \"transcription\": transcription,\n",
    "            \"audio_path\": audio_path,\n",
    "            \"label\": label,    \n",
    "            \"next_label\": next_label,   \n",
    "            \"index\": index       \n",
    "        })            \n",
    "        \n",
    "        \n",
    "    \n",
    "    sequence_data = create_sequence_pairs(data)\n",
    "    \n",
    "    pkl_path = os.path.join(output_dir, \"iemocap_preprocessed_data.pkl\")\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(sequence_data, f)\n",
    "    print(f\"Data preprocessed and saved to {pkl_path}\")\n",
    "    return\n",
    "\n",
    "\n",
    "df_iemocap = pd.read_csv(TRANSCRIPT_CSV)  # Load the transcript CSV\n",
    "#df_iemocap.tail()\n",
    "test_preprocess_iemocap(df_iemocap, OUTPUT_PKL_DIR)\n",
    "loaded_data = load_preprocessed_data(OUTPUT_PKL_DIR)\n",
    "\n",
    "if loaded_data:\n",
    "    example_data = loaded_data[0]\n",
    "    print(\"Example Loaded Data:\")\n",
    "    print(f\"History Length: {len(example_data['history'])}\")\n",
    "    print(f\"Current Hubert Shape: {example_data['current_hubert'].shape}\")\n",
    "    print(f\"Current Text Shape: {example_data['current_text']}\")\n",
    "    print(f\"Current Label: {example_data['current_label']}\")\n",
    "    print(f\"Current Next Label: {example_data['current_next_label']}\")\n",
    "else:\n",
    "    print(\"No preprocessed data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all file\n",
    "def preprocess_iemocap(df_iemocap, output_dir):\n",
    "    \"\"\"Preprocess data using audio filenames from the transcript CSV.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = []  \n",
    "  \n",
    "    for _, row in tqdm(df_iemocap.iterrows()):\n",
    "        # get audio filename and audio file full path\n",
    "        audio_filename = row['wav_file']\n",
    "        audio_subpath = audio_filename[:audio_filename.rfind(\"_\")]\n",
    "        audio_path = IEMOCAP_DIR +\"/Session\" + audio_filename[4]+ \"/sentences/wav/\"+ audio_subpath + \"/\"+ audio_filename+\".wav\"\n",
    "        #audio_path = os.path.join(IEMOCAP_DIR, \"Session\" + audio_filename[3], \"sentences\", \"wav\", audio_filename)  # Reconstruct audio path.\n",
    "  \n",
    "        if os.path.exists(audio_path):\n",
    "\n",
    "            print(\"audio_path:\",audio_path)\n",
    "            transcription = get_transcription(audio_filename, df_iemocap)\n",
    "            hubert_features  = extract_audio_features(audio_path)  #TODO: change to WavLM            \n",
    "\n",
    "            text_tokens = roberta_tokenizer(transcription, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            text_tokens = {key: value.squeeze(0).cpu().numpy() for key, value in text_tokens.items()}            \n",
    "            label = row['label'] \n",
    "            index = row['index']\n",
    "            next_label = row['next_label'] \n",
    "            #print(\"label\",label)\n",
    "\n",
    "            data.append({\n",
    "                \"audio_file\": audio_filename,\n",
    "                \"hubert_features\": hubert_features,               \n",
    "                \"input_ids\": text_tokens[\"input_ids\"],\n",
    "                \"attention_mask\": text_tokens[\"attention_mask\"],\n",
    "                \"transcription\": transcription,\n",
    "                \"audio_path\": audio_path,\n",
    "                \"label\": label,\n",
    "                \"next_label\": next_label,   \n",
    "                \"index\": index       \n",
    "            })            \n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found at {audio_path}\")\n",
    "\n",
    "    sequence_data = create_sequence_pairs(data)\n",
    "\n",
    "    pkl_path = os.path.join(output_dir, \"preprocessed_data.pkl\")\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(sequence_data, f)\n",
    "    print(f\"Data preprocessed and saved to {pkl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F000.wav\n",
      "Ses01F_impro01_F000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M000.wav\n",
      "Ses01F_impro01_M000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F001.wav\n",
      "Ses01F_impro01_F001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M001.wav\n",
      "Ses01F_impro01_M001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F002.wav\n",
      "Ses01F_impro01_F002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:02,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M002.wav\n",
      "Ses01F_impro01_M002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:02,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F003.wav\n",
      "Ses01F_impro01_F003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:03,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F004.wav\n",
      "Ses01F_impro01_F004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:03,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M003.wav\n",
      "Ses01F_impro01_M003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:04,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F005.wav\n",
      "Ses01F_impro01_F005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:04,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M004.wav\n",
      "Ses01F_impro01_M004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:05,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M005.wav\n",
      "Ses01F_impro01_M005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:06,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F006.wav\n",
      "Ses01F_impro01_F006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:07,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: ../../dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M006.wav\n",
      "Ses01F_impro01_M006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:08,  1.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_iemocap \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(TRANSCRIPT_CSV)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpreprocess_iemocap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_iemocap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_PKL_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m load_preprocessed_data(OUTPUT_PKL_DIR)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loaded_data:\n",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m, in \u001b[0;36mpreprocess_iemocap\u001b[1;34m(df_iemocap, output_dir)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_path:\u001b[39m\u001b[38;5;124m\"\u001b[39m,audio_path)\n\u001b[0;32m     17\u001b[0m transcription \u001b[38;5;241m=\u001b[39m get_transcription(audio_filename, df_iemocap)\n\u001b[1;32m---> 18\u001b[0m hubert_features  \u001b[38;5;241m=\u001b[39m \u001b[43mextract_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#TODO: change to WavLM            \u001b[39;00m\n\u001b[0;32m     20\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m roberta_tokenizer(transcription, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m text_tokens\u001b[38;5;241m.\u001b[39mitems()}            \n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mextract_audio_features\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m audio_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(audio)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mhubert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:1354\u001b[0m, in \u001b[0;36mHubertModel.forward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[0;32m   1352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices)\n\u001b[1;32m-> 1354\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1362\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:995\u001b[0m, in \u001b[0;36mHubertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    988\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    989\u001b[0m             layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    990\u001b[0m             hidden_states,\n\u001b[0;32m    991\u001b[0m             attention_mask,\n\u001b[0;32m    992\u001b[0m             output_attentions,\n\u001b[0;32m    993\u001b[0m         )\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 995\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:849\u001b[0m, in \u001b[0;36mHubertEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    846\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    848\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 849\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[0;32m    852\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:815\u001b[0m, in \u001b[0;36mHubertFeedForward.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 815\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    817\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_dropout(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_iemocap = pd.read_csv(TRANSCRIPT_CSV)\n",
    "preprocess_iemocap(df_iemocap, OUTPUT_PKL_DIR)\n",
    "loaded_data = load_preprocessed_data(OUTPUT_PKL_DIR)\n",
    "\n",
    "if loaded_data:\n",
    "    example_data = loaded_data[0]\n",
    "    print(\"Example Loaded Data:\")\n",
    "    print(f\"History Length: {len(example_data['history'])}\")\n",
    "    print(f\"Current Hubert Shape: {example_data['current_hubert'].shape}\")\n",
    "    print(f\"Current Text Shape: {example_data['current_text']}\")\n",
    "    print(f\"Current Label: {example_data['current_label']}\")\n",
    "    print(f\"Current Next Label: {example_data['current_next_label']}\")\n",
    "else:\n",
    "    print(\"No preprocessed data loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

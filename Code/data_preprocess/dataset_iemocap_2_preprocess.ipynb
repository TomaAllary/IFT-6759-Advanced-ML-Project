{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liyua\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#In this file, \n",
    "Go through the  df_iemocap_recognition.csv file, for every row, extract following data , store the data in a csv file.:\n",
    "\n",
    "\"audio_file\": audio file name,\n",
    "\"audio_path\": audio file full path,\n",
    "\"transcription\": current text,\n",
    "\"audio_features\": features extract from audio  by Robert\n",
    "\"input_ids\" and \"attention_mask\" : features extract from text by Robert\n",
    "\"label\": emotion label of current sentence\n",
    "\n",
    "\n",
    "TODO:\n",
    "1. use future emotion Label as for prediction\n",
    "2. add context (3 sentences), use current sentence and previous 2 sentences for prediction\n",
    "3. change to store in .pkl file\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, HubertModel, HubertConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HubertModel(\n",
       "  (feature_extractor): HubertFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): HubertGroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x HubertNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x HubertNoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): HubertFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): HubertEncoder(\n",
       "    (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): HubertSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x HubertEncoderLayer(\n",
       "        (attention): HubertSdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): HubertFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "IEMOCAP_DIR = \"dataset/IEMOCAP_full_release\"  \n",
    "SESSIONS = [\"Session1\", \"Session2\", \"Session3\", \"Session4\", \"Session5\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUTPUT_CSV_DIR = \"data/preprocess/\"\n",
    "TRANSCRIPT_CSV = \"data/df_iemocap_recognition.csv\"  \n",
    "\n",
    "# --- Models and Tokenizers ---\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "hubert_config = HubertConfig.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\").to(DEVICE)\n",
    "hubert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Hubert to extract audio feature\n",
    "def extract_audio_features(audio_path):\n",
    "    \"\"\"Extract HuBERT features from audio.\"\"\"\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    audio_tensor = torch.tensor(audio).unsqueeze(0).to(DEVICE).float()\n",
    "    with torch.no_grad():\n",
    "        outputs = hubert_model(audio_tensor)\n",
    "    return outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "\n",
    "#  get transcript for audio file\n",
    "def get_transcription(audio_file_path, transcript_df):\n",
    "    \"\"\"Get transcription from the provided CSV DataFrame.\"\"\"\n",
    "    audio_filename = os.path.basename(audio_file_path)\n",
    "    print (audio_filename)\n",
    "    if audio_filename in transcript_df['wav_file'].values:\n",
    "        return transcript_df.loc[transcript_df['wav_file'] == audio_filename, 'script'].iloc[0]\n",
    "    else:\n",
    "        print(f\"Warning: Transcription not found for {audio_filename}\")\n",
    "        return \"Transcription not found.\"  # Or handle missing transcript as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(csv_dir):\n",
    "    \"\"\"Load preprocessed data from CSV files.\"\"\"\n",
    "    all_data = []\n",
    "  \n",
    "    for file in os.listdir(csv_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(csv_dir, file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            for _, row in df.iterrows():\n",
    "                all_data.append({\n",
    "                    \"audio_file\": row[\"audio_file\"],\n",
    "                    \"audio_features\": np.array(eval(row[\"audio_features\"])), #convert string back to list, then to numpy array.                    \n",
    "                    \"input_ids\": np.array(eval(row[\"input_ids\"])),\n",
    "                    \"attention_mask\": np.array(eval(row[\"attention_mask\"])),\n",
    "                    \"transcription\": row[\"transcription\"],\n",
    "                    \"audio_path\": row[\"audio_path\"]\n",
    "                })\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Loaded Data:\n",
      "Audio Features Shape: (97, 768)\n",
      "Input IDs Shape: (6,)\n",
      "Transcription: Excuse me.\n",
      "Audio Path: dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F000.wav\n"
     ]
    }
   ],
   "source": [
    "# test on one file\n",
    "def test_preprocess_iemocap(df_script, output_dir):\n",
    "    \"\"\"Preprocess data using audio filenames from the transcript CSV.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = []\n",
    "    \n",
    "    row = df_script.iloc[0]\n",
    "    audio_filename = row['wav_file']\n",
    "    audio_subpath = audio_filename[:audio_filename.rfind(\"_\")]\n",
    "    audio_path = IEMOCAP_DIR +\"/Session\" + audio_filename[4]+ \"/sentences/wav/\"+ audio_subpath + \"/\"+ audio_filename+\".wav\"\n",
    "    #audio_path = os.path.join(IEMOCAP_DIR, \"Session\" + audio_filename[3], \"sentences\", \"wav\", audio_filename)  # Reconstruct audio path.\n",
    "        \n",
    "    if os.path.exists(audio_path):       \n",
    "        transcription = get_transcription(audio_filename, df_script)       \n",
    "        audio_features = extract_audio_features(audio_path)\n",
    "        #print(\"audio_features\",audio_features)        \n",
    "        print(f\"Shape of audio_features: {audio_features.shape}\")\n",
    "        print(f\"Data type of audio_features: {audio_features.dtype}\")       \n",
    "        flattened_list=audio_features.flatten().tolist()\n",
    "        print(f\"First 10 elements of flattened audio features: {flattened_list[:10]}\")       \n",
    "        text_tokens = roberta_tokenizer(transcription, padding=True, truncation=True, return_tensors=\"pt\")        \n",
    "        text_tokens = {key: value.squeeze(0).cpu().numpy() for key, value in text_tokens.items()}        \n",
    "        #label = row['emotion'] if 'label' in df_script.columns else None\n",
    "        label = row['emotion']         \n",
    "        \n",
    "        data.append({\n",
    "            \"audio_file\": audio_filename,\n",
    "            \"audio_features\": flattened_list,        \n",
    "            \"input_ids\": text_tokens[\"input_ids\"].flatten().tolist(),\n",
    "            \"attention_mask\": text_tokens[\"attention_mask\"].flatten().tolist(),\n",
    "            \"transcription\": transcription,\n",
    "            \"audio_path\": audio_path,\n",
    "            \"label\": label\n",
    "        })\n",
    "        \n",
    "        #print(\"data\",data)   can't print because audio_features list is too big\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(output_dir, \"preprocessed_data.csv\")  \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Data preprocessed and saved to {csv_path}\")\n",
    "    return\n",
    "\n",
    "\n",
    "df_script = pd.read_csv(TRANSCRIPT_CSV)  # Load the transcript CSV\n",
    "test_preprocess_iemocap(df_script, OUTPUT_CSV_DIR)\n",
    "loaded_data = load_preprocessed_data(OUTPUT_CSV_DIR)\n",
    "\n",
    "if loaded_data:\n",
    "    example_data = loaded_data[0]\n",
    "    print(\"Example Loaded Data:\")     \n",
    "    flattened_list =example_data['audio_features']\n",
    "    original_shape=(97, 768)    \n",
    "    reshaped_array =np.array(flattened_list).reshape(original_shape)\n",
    "    print(f\"Audio Features Shape: {reshaped_array.shape}\")  \n",
    "    print(f\"Input IDs Shape: {example_data['input_ids'].shape}\")\n",
    "    print(f\"Transcription: {example_data['transcription']}\")\n",
    "    print(f\"Audio Path: {example_data['audio_path']}\")\n",
    "else:\n",
    "    print(\"No preprocessed data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all file\n",
    "def preprocess_iemocap(df_script, output_dir):\n",
    "    \"\"\"Preprocess data using audio filenames from the transcript CSV.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = []  \n",
    "    \n",
    "    for _, row in tqdm(df_script.iterrows()):\n",
    "        # get audio filename and audio file full path\n",
    "        audio_filename = row['wav_file']\n",
    "        audio_subpath = audio_filename[:audio_filename.rfind(\"_\")]\n",
    "        audio_path = IEMOCAP_DIR +\"/Session\" + audio_filename[4]+ \"/sentences/wav/\"+ audio_subpath + \"/\"+ audio_filename+\".wav\"\n",
    "        #audio_path = os.path.join(IEMOCAP_DIR, \"Session\" + audio_filename[3], \"sentences\", \"wav\", audio_filename)  # Reconstruct audio path.\n",
    "        \n",
    "        \n",
    "        if os.path.exists(audio_path):\n",
    "            print(\"audio_path:\",audio_path)\n",
    "            transcription = get_transcription(audio_filename, df_script)\n",
    "            audio_features = extract_audio_features(audio_path)\n",
    "            text_tokens = roberta_tokenizer(transcription, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            text_tokens = {key: value.squeeze(0).cpu().numpy() for key, value in text_tokens.items()}\n",
    "            #label = row['emotion'] if 'label' in df_script.columns else None\n",
    "            label = row['emotion'] \n",
    "            print(\"label\",label)\n",
    "\n",
    "            data.append({\n",
    "                \"audio_file\": audio_filename,\n",
    "                \"audio_features\": audio_features.flatten().tolist(),\n",
    "                \"input_ids\": text_tokens[\"input_ids\"].flatten().tolist(),\n",
    "                \"attention_mask\": text_tokens[\"attention_mask\"].flatten().tolist(),\n",
    "                \"transcription\": transcription,\n",
    "                \"audio_path\": audio_path,\n",
    "                \"label\": label\n",
    "            })            \n",
    "        else:\n",
    "            print(f\"Warning: Audio file not found at {audio_path}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(output_dir, \"preprocessed_data.csv\")  # Single CSV for all\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Data preprocessed and saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path: dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F000.wav\n",
      "Ses01F_impro01_F000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label neu\n",
      "audio_path: dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M000.wav\n",
      "Ses01F_impro01_M000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label fru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessed and saved to data/preprocess/preprocessed_data.csv\n",
      "Example Loaded Data:\n",
      "Audio Features Shape: (74496,)\n",
      "Input IDs Shape: (6,)\n",
      "Transcription: Excuse me.\n",
      "Audio Path: dataset/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F000.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_script = pd.read_csv(TRANSCRIPT_CSV)  # Load the transcript CSV\n",
    "preprocess_iemocap(df_script, OUTPUT_CSV_DIR)\n",
    "loaded_data = load_preprocessed_data(OUTPUT_CSV_DIR)\n",
    "\n",
    "# Example: Accessing loaded data\n",
    "if loaded_data:\n",
    "    example_data = loaded_data[0]\n",
    "    print(\"Example Loaded Data:\")\n",
    "    print(f\"Audio Features Shape: {example_data['audio_features'].shape}\")\n",
    "    print(f\"Input IDs Shape: {example_data['input_ids'].shape}\")\n",
    "    print(f\"Transcription: {example_data['transcription']}\")\n",
    "    print(f\"Audio Path: {example_data['audio_path']}\")\n",
    "else:\n",
    "    print(\"No preprocessed data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
